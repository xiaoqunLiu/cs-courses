{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Objective</a></span><ul class=\"toc-item\"><li><span><a href=\"#Drawbacks-of-k-means-Clustering\" data-toc-modified-id=\"Drawbacks-of-k-means-Clustering-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Drawbacks of k-means Clustering</a></span></li><li><span><a href=\"#Gaussian-mixture-model-(GMM)\" data-toc-modified-id=\"Gaussian-mixture-model-(GMM)-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Gaussian mixture model (GMM)</a></span></li><li><span><a href=\"#The-Gaussian-Distribution\" data-toc-modified-id=\"The-Gaussian-Distribution-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>The Gaussian Distribution</a></span></li><li><span><a href=\"#Gaussian-Mixture-Models\" data-toc-modified-id=\"Gaussian-Mixture-Models-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Gaussian Mixture Models</a></span></li><li><span><a href=\"#Expectation-Maximization-(EM)-Algorithm\" data-toc-modified-id=\"Expectation-Maximization-(EM)-Algorithm-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Expectation-Maximization (EM) Algorithm</a></span></li></ul></li><li><span><a href=\"#LAB-Assignment\" data-toc-modified-id=\"LAB-Assignment-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>LAB Assignment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise-(100-Points)\" data-toc-modified-id=\"Exercise-(100-Points)-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Exercise (100 Points)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-some-libraries\" data-toc-modified-id=\"Import-some-libraries-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Import some libraries</a></span></li><li><span><a href=\"#Load-Image\" data-toc-modified-id=\"Load-Image-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Load Image</a></span></li><li><span><a href=\"#Initialize-means,-covariance-matrices-and-mixing-coefficients-of-GMM\" data-toc-modified-id=\"Initialize-means,-covariance-matrices-and-mixing-coefficients-of-GMM-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Initialize means, covariance matrices and mixing coefficients of GMM</a></span></li><li><span><a href=\"#Implement-GMM-algorithm\" data-toc-modified-id=\"Implement-GMM-algorithm-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>Implement GMM algorithm</a></span><ul class=\"toc-item\"><li><span><a href=\"#E-step\" data-toc-modified-id=\"E-step-2.1.4.1\"><span class=\"toc-item-num\">2.1.4.1&nbsp;&nbsp;</span>E-step</a></span></li><li><span><a href=\"#M-step\" data-toc-modified-id=\"M-step-2.1.4.2\"><span class=\"toc-item-num\">2.1.4.2&nbsp;&nbsp;</span>M-step</a></span></li></ul></li><li><span><a href=\"#Iteration\" data-toc-modified-id=\"Iteration-2.1.5\"><span class=\"toc-item-num\">2.1.5&nbsp;&nbsp;</span>Iteration</a></span></li><li><span><a href=\"#Display\" data-toc-modified-id=\"Display-2.1.6\"><span class=\"toc-item-num\">2.1.6&nbsp;&nbsp;</span>Display</a></span></li><li><span><a href=\"#sample-Result\" data-toc-modified-id=\"sample-Result-2.1.7\"><span class=\"toc-item-num\">2.1.7&nbsp;&nbsp;</span>sample Result</a></span></li></ul></li><li><span><a href=\"#Questions(3-points)\" data-toc-modified-id=\"Questions(3-points)-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Questions(3 points)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB12 tutorial for Machine Learning <br > Clustering with GMM\n",
    "> The document description are designed by JIa Yanhong in 2022. Nov. 21th\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "- Understand GMM clustering algorithm theory\n",
    "- Implement the GMM clustering algorithm  from scratch in python\n",
    "- Complete the LAB assignment.\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawbacks of k-means Clustering\n",
    "\n",
    "The k-means clustering concept sounds pretty great, right? It’s simple to understand, relatively easy to implement, and can be applied in quite a number of use cases. But there are certain drawbacks and limitations that we need to be aware of. <font color=red>K-means often doesn't work when clusters are not round shaped</font>\n",
    "\n",
    "First, KMeans doesn't put data points that are far away from each other into the same cluster, even when they obviously should be because they underly some obvious structure like points on a line, for example.\n",
    "\n",
    "\n",
    "<div  align=\"center\"> <img src=\"images/two_lines.png\"  width=400 align=center /></div>\n",
    "\n",
    "\n",
    "\n",
    "Second, KMeans performs poorly for complicated geometric shapes such as the moons and circles shown below.\n",
    "\n",
    "\n",
    "<div  align=\"center\"> \n",
    "<img src=\"images/noisy_moons_with_true_output.png\"  width=400 align=center />\n",
    "<img src=\"images/noisy_circles_with_true_output.png\"  width=400 align=center />\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "In addition,k-means doesn't work when clusters are may overlap.\n",
    "<div  align=\"center\"> <img src=\"images/image-20221121170553059.png\"  width=200 align=center /></div>\n",
    "\n",
    "\n",
    "Hence, we need a different way to assign clusters to the data points.  So instead of using a distance-based model, we will now use a distribution-based model.  And that is where `Gaussian Mixture Models` come into this lab!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian mixture model (GMM)\n",
    "\n",
    "Gaussian Mixture Models (GMMs) assume that there are a certain number of Gaussian distributions, and each of these distributions represent a cluster.\n",
    "\n",
    "**Gaussian Mixture Models are probabilistic models and use the soft clustering approach for distributing the points in different clusters.** \n",
    "\n",
    " Let us take an example that will make it easier to understand.\n",
    "\n",
    "Here, we have three clusters that are denoted by three colors – Blue, Green, and Cyan. Let’s take the data point highlighted in red. The probability of this point being a part of the blue cluster is 1, while the probability of it being a part of the green or cyan clusters is 0.\n",
    "\n",
    "\n",
    "<div  align=\"center\"> <img src=\"images/Screenshot-from-2019-10-21-12-52-06.png\"  width=400 align=center /></div>\n",
    "Now, consider another point – somewhere in between the blue and cyan (highlighted in the below figure). The probability that this point is a part of cluster green is 0, right? And the probability that this belongs to blue and cyan is 0.2 and 0.8 respectively.\n",
    "\n",
    "<div  align=\"center\"> <img src=\"images/Screenshot-from-2019-10-21-12-53-29.png\"  width=400 align=center /></div>\n",
    "\n",
    "\n",
    "Gaussian Mixture Models use the soft clustering technique for assigning data points to Gaussian distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Gaussian Distribution\n",
    "\n",
    "In a one dimensional space, the **probability density function** of a Gaussian distribution is given by:\n",
    "\n",
    "\n",
    "\n",
    "$$\\mathcal{N}(X|\\mu, \\sigma)=\\frac{1}{{\\sigma \\sqrt {2\\pi } }}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "where μ is the mean and $\\sigma^{2}$ is the variance.\n",
    "\n",
    "The below image has a few Gaussian distributions with a difference in mean (μ) and variance (σ2).\n",
    "\n",
    "\n",
    "<div  align=\"center\"> <img src=\"images/gaussians.png\"  width=400 align=center /></div>\n",
    "\n",
    "But this would only be true for a single variable. In the case of two variables, instead of a 2D bell-shaped curve, we will have a 3D bell curve as shown below:\n",
    "\n",
    "<div  align=\"center\"> <img src=\"images/gaussians-3d-166902260287820.png\"  width=400 align=center /></div>\n",
    "\n",
    "The probability density function would be given by:\n",
    "\n",
    "\n",
    "\n",
    "$$\\mathcal{N}(X|\\mu, \\Sigma)= \\frac{1}{\\sqrt{(2\\pi)|\\boldsymbol\\Sigma|}} \\exp\\left(-\\frac{1}{2}({X}-{\\mu})^T{\\boldsymbol\\Sigma}^{-1}({X}-{\\mu}) \\right)    $$ \n",
    "\n",
    "\n",
    "\n",
    "where $X$ is the input vector, μ is the 2D mean vector, and Σ is the 2×2 covariance matrix. The covariance would now define the shape of this curve. We can generalize the same for d-dimensions.\n",
    "\n",
    "Thus, this multivariate Gaussian model would have $X$ and $\\mu$ as vectors of length d, and Σ would be a *d x d* covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Models\n",
    "\n",
    "Suppose there are K clusters (For the sake of simplicity here it is assumed that the number of clusters is known and it is K). So $\\mu$ and $ \\Sigma$ are also estimated for each k. Had it been only one distribution, they would have been estimated by the **maximum-likelihood method**. But since there are K such clusters and the probability density is defined as a linear function of densities of all these K distributions, i.e.\n",
    "\n",
    "$$p(\\mathbf{x}) =\\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf{x}|\\mu_k, \\Sigma_k) \n",
    "\\\\\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "&\\pi: \\text{mixing coefficient}\\\\\n",
    "&\\pmb{\\mu}: \\text{means}\\\\\n",
    "&\\pmb{\\Sigma}: \\text{covariance matrix}\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "where $\\pi_k$   is the mixing coefficient for k-th distribution.\n",
    "\n",
    "Assuming that data points are independent, for estimating the parameters by the maximum log-likelihood method, compute $\\hspace{0.25 cm} {p( \\mathbf{X}|\\mu, \\Sigma, \\pi)}$.\n",
    "\n",
    "\n",
    "$$ln\\ p( \\mathbf{X}|\\mu, \\Sigma, \\pi) =\\sum_{n=1}^N ln\\ p(\\mathbf{x}_n) =\\sum_{n=1}^N ln {\\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf{x}_n | \\mu_k, \\Sigma_k)}  $$\n",
    "\n",
    "\n",
    "\n",
    "Now define a random variable $\\gamma_k(\\mathbf{x}_n)$ , such that $\\gamma_k(\\mathbf{x}_n)  =p(k|\\mathbf{x}_n)$.\n",
    "From Bayes’ theorem, \n",
    "\n",
    "\n",
    "$$\n",
    "\\gamma_k(\\mathbf{x}_n) =\\frac{p(\\mathbf{x}_n|k)p(k)}{\\sum_{k=1}^K p(k)p(\\mathbf{x}_n|k)} \n",
    "=\\frac{p(\\mathbf{x}_n|k)\\pi_k}{\\sum_{k=1}^K \\pi_k p(\\mathbf{x}_n|k)}\n",
    "=\\frac{\\pi_k\\mathcal{N}(\\mathbf{x}_n | \\mu_k, \\Sigma_k)}{\\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf{x}_n | \\mu_k, \\Sigma_k)}$$\n",
    "\n",
    "Now for the log-likelihood function to be maximum, its derivative of $p(x_n|\\mu, \\Sigma, \\pi) $ with respect to $\\mu$, $\\Sigma $ and $\\pi $ should be zero. So equating the derivative of $p(x_n|\\mu, \\Sigma, \\pi) $ to zero and rearranging the terms, \n",
    "\n",
    "\n",
    "$$\\mu_k=\\frac{\\sum_{n=1}^N \\gamma_k(\\mathbf{x}_n)\\mathbf{x}_n}{\\sum_{n=1}^N \\gamma_k(\\mathbf{x}_n)}$$\n",
    "\n",
    "\n",
    "\n",
    "Similarly taking derivative with respect to $\\Sigma $ and pi respectively, one can obtain the following expressions.\n",
    "\n",
    "$$\\Sigma_k=\\frac{\\sum_{n=1}^N \\gamma_k(\\mathbf{x}_n)(\\mathbf{x}_n-\\mu_k)(\\mathbf{x}_n-\\mu_k)^T}{\\sum_{n=1}^N \\gamma_k(\\mathbf{x}_n)} \\newline $$\n",
    "\n",
    "And\n",
    "\n",
    "\n",
    "$$\\pi_k=\\frac{1}{N} \\sum_{n=1}^N \\gamma_k(\\mathbf{x}_n)$$\n",
    "\n",
    "\n",
    "\n",
    "**Note:** $\\sum_{n=1}^N\\gamma_k(x_n) $ denotes the total number of sample points in the k-th cluster. Here it is assumed that there is a total N number of samples and each sample containing d features is denoted by $x_i $.\n",
    "So it can be clearly seen that the parameters cannot be estimated in closed form. This is where the **Expectation-Maximization algorithm** is beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在GMM（高斯混合模型）中，这句话的意思是：$\\sum_{n=1}^N\\gamma_k(x_n)$ 表示第 k 个聚类中的样本点的总数。这里假设总共有 N 个样本，每个样本包含 d 个特征，用 $x_i$ 表示。\n",
    "\n",
    "这句话的目的是说明在 GMM 中，参数无法通过闭式形式直接估计。也就是说，无法通过简单的公式或方程来计算参数的值。这是因为在 GMM 中，参数的估计涉及到对隐藏变量（即每个样本点属于哪个聚类）的推断。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体来说，GMM中的参数包括每个聚类的均值向量（$\\mu_k$）、协方差矩阵（$\\Sigma_k$）和每个聚类的权重（$\\pi_k$）。这些参数需要根据观测数据和隐藏变量的后验概率进行估计。\n",
    "\n",
    "在EM算法的E步骤中，需要计算每个样本点属于每个聚类的后验概率（$\\gamma_k(\\mathbf{x}_n)$）。这些后验概率表示样本点属于每个聚类的概率，是隐藏变量的一种表示。在E步骤中，需要对每个样本点和每个聚类计算后验概率，这涉及到对隐藏变量进行求解。\n",
    "\n",
    "在M步骤中，根据E步骤得到的后验概率，需要更新每个聚类的均值向量、协方差矩阵和权重。具体来说，均值向量可以通过对每个样本点的加权平均得到，权重可以通过对后验概率的加权平均得到。而协方差矩阵的估计涉及到对每个样本点与均值向量之间的差的乘积进行加权平均。\n",
    "\n",
    "由于参数的估计涉及到对隐藏变量的推断和加权平均，无法通过简单的公式或方程直接计算参数的值。因此，参数的估计在GMM中无法通过闭式形式直接得到。\n",
    "\n",
    "这就是为什么需要使用Expectation-Maximization（EM）算法。EM算法通过交替进行E步骤和M步骤的迭代，逐步优化参数的估计，直到达到收敛。通过EM算法，可以在存在隐藏变量的概率模型中进行参数估计，从而实现对数据的聚类和概率建模。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation-Maximization (EM) Algorithm\n",
    "\n",
    "The Expectation-Maximization (EM) algorithm is an iterative way to find maximum-likelihood estimates for model parameters when the data is incomplete or has some missing data points or has some hidden variables. EM chooses some random values for the missing data points and estimates a new set of data. These new values are then recursively used to estimate a better first date, by filling up missing points, until the values get fixed. \n",
    "These are the two basic steps of the EM algorithm, namely **E Step or Expectation Step or Estimation Step** and **M Step or Maximization Step**.\n",
    "\n",
    "\n",
    "- Estimation step (E step):\n",
    "  - initialize $\\mu_k   $, $\\Sigma_k   $ and $\\pi_k  $ by some random values, or by K means clustering results or by hierarchical clustering results.\n",
    "  - Then for those given parameter values, estimate the value of the latent variables (i.e $\\gamma_k(\\mathbf{x}_n)   $)\n",
    "  $$\\gamma_k(\\mathbf{x}_n) =\\frac{\\pi_k\\mathcal{N}(\\mathbf{x}_n | \\mu_k, \\Sigma_k)}{\\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf{x}_n | \\mu_k, \\Sigma_k)}$$\n",
    "- Maximization Step(M step):\n",
    "  - Update the value of the parameters( i.e. $\\mu_k    $, $\\Sigma_k     $ and $\\pi_k    $) calculated using ML method.\n",
    "  $$\\mu_k=\\frac{\\sum_{n=1}^N \\gamma_k(\\mathbf{x}_n)\\mathbf{x}_n}{\\sum_{n=1}^N \\gamma_k(\\mathbf{x}_n)}$$\n",
    "  $$\\Sigma_k=\\frac{\\sum_{n=1}^N \\gamma_k(\\mathbf{x}_n)(\\mathbf{x}_n-\\mu_k)(\\mathbf{x}_n-\\mu_k)^T}{\\sum_{n=1}^N \\gamma_k(\\mathbf{x}_n)} \\newline $$\n",
    "  $$\\pi_k=\\frac{1}{N} \\sum_{n=1}^N \\gamma_k(\\mathbf{x}_n)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB Assignment\n",
    "Please finish the **Exercise** and answer **Questions**.\n",
    "### Exercise (100 Points)\n",
    "In this lab, our goal is to write a program to segment different objects using the **GMM and EM** algorithm. We also use <u>*k-means* clustering algorithm to initialize the parameters</u> of GMM. The following steps should be implemented to achieve such a goal:\n",
    "\n",
    "1. Load image\n",
    "2. Initialize parameters of GMM using K-means\n",
    "3. Implement the EM algorithm for GMM\n",
    "4. Display result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unfinished = True\n",
    "do not need to be finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.cluster import KMeans\n",
    "import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "COLORS = [\n",
    "    (255, 0, 0),   # red\n",
    "    (0, 255, 0),  # green\n",
    "    (0, 0, 255),   # blue\n",
    "    (255, 255, 0), # yellow\n",
    "    (255, 0, 255), # magenta\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Image\n",
    "What you should do is to implement Z-score normalization in `load()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def load(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    h, w, c = image.shape\n",
    "\n",
    "    # TODO: please normalize image_pixl using Z-score\n",
    "    _mean = np.mean(image, axis=(0, 1))\n",
    "    _std = np.std(image, axis=(0, 1))\n",
    "    image_norm = ((image - _mean) / _std).astype(np.float32)\n",
    "    \n",
    "    return h, w, c, image_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize means, covariance matrices and mixing coefficients of GMM\n",
    "k-means is used to initialize means, covariance matrices and mixing coefficients of GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(n_cluster, image_pixl):\n",
    "    kmeans = KMeans(n_clusters=n_cluster)# instantiate a K-means\n",
    "    labels = kmeans.fit_predict(image_pixl)# fit and get clustering result\n",
    "    initial_mus = kmeans.cluster_centers_# get centroids\n",
    "    initial_priors, initial_covs = [], []\n",
    "    #Followings are for initialization:\n",
    "    for i in range(n_cluster):\n",
    "        datas = image_pixl[labels == i, ...].T\n",
    "        initial_covs.append(np.cov(datas))\n",
    "        initial_priors.append(datas.shape[1] / len(labels))\n",
    "    return initial_mus, initial_priors, initial_covs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement GMM algorithm\n",
    "We use EM algorithm to refine GMM's parameters.\n",
    "\n",
    "Although it may be not easy for some students to derive EM formula for GMM, GMM isn't very difficult to implement once you have the formula. Therefore, to help you understand GMM more, there are still some blanks for you to fill in.\n",
    "\n",
    "<div  align=\"center\"> <img src=\"images/EM-step.png\"  /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### E-step\n",
    "It is in `inference()`.\n",
    "\n",
    "In the following code, `prob` is $\\pi_k\\mathcal{N}(\\mathbf{x}_n|\\pmb{\\mu}_k,\\pmb\\Sigma_k)$, `gamma` is  $\\gamma$. You need to implement log likelihood and $\\gamma$.\n",
    "```python\n",
    "def inference(self, datas):\n",
    "    probs = []\n",
    "    for i in range(self.ncomp):\n",
    "        mu, cov, prior = self.mus[i, :], self.covs[i, :, :], self.priors[i]\n",
    "        prob = prior * multivariate_normal.pdf(datas, mean=mu, cov=cov, allow_singular=True)\n",
    "        probs.append(np.expand_dims(prob, -1))\n",
    "    preds = np.concatenate(probs, axis=1)\n",
    "    \n",
    "    # TODO: calc log likelihood\n",
    "    log_likelihood = None\n",
    "\n",
    "    # TODO: calc gamma\n",
    "    gamma = None\n",
    "\n",
    "    return gamma, log_likelihood\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### M-step\n",
    "It is in `update()`\n",
    "\n",
    "You need to implement mean $\\mu$, covariance $\\Sigma$ and mixing coefficient $\\pi$ .\n",
    "```python\n",
    "def update(self, datas, gamma):\n",
    "    new_mus, new_covs, new_priors = [], [], []\n",
    "    soft_counts = np.sum(gamma, axis=0)\n",
    "    for i in range(self.ncomp):\n",
    "        # TODO: calc mu\n",
    "        new_mu = None\n",
    "        new_mus.append(new_mu)\n",
    "\n",
    "        # TODO: calc cov\n",
    "        new_cov = None\n",
    "        new_covs.append(new_cov)\n",
    "\n",
    "        # TODO: calc mixing coefficients\n",
    "        new_prior = None\n",
    "        new_priors.append(new_prior)\n",
    "\n",
    "    self.mus = np.asarray(new_mus)\n",
    "    self.covs = np.asarray(new_covs)\n",
    "    self.priors = np.asarray(new_priors)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration\n",
    "Iteration part is as you see in `fit()`\n",
    "``` python\n",
    "def fit(self, data, iteration):\n",
    "    prev_log_liklihood = None\n",
    "\n",
    "    bar = tqdm.tqdm(total=iteration)\n",
    "    for i in range(iteration):\n",
    "        gamma, log_likelihood = self.inference(data)\n",
    "        self.update(data, gamma)\n",
    "        if prev_log_liklihood is not None and abs(log_likelihood - prev_log_liklihood) < 1e-10:\n",
    "            break\n",
    "        prev_log_likelihood = log_likelihood\n",
    "\n",
    "        bar.update()\n",
    "        bar.set_postfix({\"log likelihood\": log_likelihood})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM:\n",
    "    def __init__(self, ncomp, initial_mus, initial_covs, initial_priors):\n",
    "        \"\"\"\n",
    "        :param ncomp:           the number of clusters\n",
    "        :param initial_mus:     initial means\n",
    "        :param initial_covs:    initial covariance matrices\n",
    "        :param initial_priors:  initial mixing coefficients\n",
    "        \"\"\"\n",
    "        self.ncomp = ncomp\n",
    "        self.mus = np.asarray(initial_mus)\n",
    "        self.covs = np.asarray(initial_covs)\n",
    "        self.priors = np.asarray(initial_priors)\n",
    "\n",
    "    def inference(self, datas):\n",
    "        \"\"\"\n",
    "        E-step\n",
    "        :param datas:   original data\n",
    "        :return:        posterior probability (gamma) and log likelihood\n",
    "        \"\"\"\n",
    "        probs = []\n",
    "        for i in range(self.ncomp):\n",
    "            mu, cov, prior = self.mus[i, :], self.covs[i, :, :], self.priors[i]\n",
    "            prob = prior * multivariate_normal.pdf(datas, mean=mu, cov=cov, allow_singular=True)\n",
    "            probs.append(np.expand_dims(prob, -1))\n",
    "        preds = np.concatenate(probs, axis=1)\n",
    "\n",
    "        # calc log likelihood\n",
    "        log_likelihood = np.sum(np.log(np.sum(preds, axis=1)))\n",
    "\n",
    "        # calc gamma\n",
    "        gamma = preds / np.sum(preds, axis=1, keepdims=True)\n",
    "        gamma = np.nan_to_num(gamma, nan=1. / self.ncomp)\n",
    "        gamma = gamma / np.sum(gamma, axis=0, keepdims=True)\n",
    "\n",
    "        return gamma, log_likelihood\n",
    "\n",
    "    def update(self, datas, gamma):\n",
    "        \"\"\"\n",
    "        M-step\n",
    "        :param datas:   original data\n",
    "        :param gamma:    gamma\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        new_mus, new_covs, new_priors = [], [], []\n",
    "        soft_counts = np.sum(gamma, axis=0)\n",
    "        for i in range(self.ncomp):\n",
    "            # calc mu\n",
    "            new_mu = soft_counts[i] * np.sum(gamma[:, i:i + 1] * datas, axis=0) / np.sum(gamma[:, i])\n",
    "            new_mus.append(new_mu)\n",
    "\n",
    "            # TODO: calc cov\n",
    "            new_cov = None\n",
    "            new_covs.append(new_cov)\n",
    "\n",
    "            # TODO: calc mixing coefficients\n",
    "            new_prior = None\n",
    "            new_priors.append(new_prior)\n",
    "\n",
    "        self.mus = np.asarray(new_mus)\n",
    "        self.covs = np.asarray(new_covs)\n",
    "        self.priors = np.asarray(new_priors)\n",
    "\n",
    "    def fit(self, data, iteration):\n",
    "        prev_log_liklihood = None\n",
    "\n",
    "        bar = tqdm.tqdm(total=iteration)\n",
    "        for i in range(iteration):\n",
    "            gamma, log_likelihood = self.inference(data)\n",
    "            self.update(data, gamma)\n",
    "            if prev_log_liklihood is not None and abs(log_likelihood - prev_log_liklihood) < 1e-10:\n",
    "                break\n",
    "            prev_log_likelihood = log_likelihood\n",
    "\n",
    "            bar.update()\n",
    "            bar.set_postfix({\"log likelihood\": log_likelihood})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display\n",
    "We use `matplotlib` to display what we segment, you can check the code in `visualize()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualize(gmm, image, ncomp, ih, iw):\n",
    "    beliefs, log_likelihood = gmm.inference(image)\n",
    "    map_beliefs = np.reshape(beliefs, (ih, iw, ncomp))\n",
    "    segmented_map = np.zeros((ih, iw, 3))\n",
    "    for i in range(ih):\n",
    "        for j in range(iw):\n",
    "            hard_belief = np.argmax(map_beliefs[i, j, :])\n",
    "            segmented_map[i, j, :] = np.asarray(COLORS[hard_belief]) / 255.0\n",
    "    plt.imshow(segmented_map)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=nan.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\My_university\\temp\\CS405ML\\lab\\12GMM\\Lab12.GMM Cluster.ipynb 单元格 23\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/My_university/temp/CS405ML/lab/12GMM/Lab12.GMM%20Cluster.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m iteration\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/My_university/temp/CS405ML/lab/12GMM/Lab12.GMM%20Cluster.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# init mu, prior and cov\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/My_university/temp/CS405ML/lab/12GMM/Lab12.GMM%20Cluster.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m initial_mus, initial_priors, initial_covs \u001b[39m=\u001b[39m kmeans(ncomp, image_norm)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/My_university/temp/CS405ML/lab/12GMM/Lab12.GMM%20Cluster.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# GMM\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/My_university/temp/CS405ML/lab/12GMM/Lab12.GMM%20Cluster.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGMM begins...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32md:\\My_university\\temp\\CS405ML\\lab\\12GMM\\Lab12.GMM Cluster.ipynb 单元格 23\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/My_university/temp/CS405ML/lab/12GMM/Lab12.GMM%20Cluster.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mkmeans\u001b[39m(n_cluster, image_pixl):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/My_university/temp/CS405ML/lab/12GMM/Lab12.GMM%20Cluster.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     kmeans \u001b[39m=\u001b[39m KMeans(n_clusters\u001b[39m=\u001b[39mn_cluster)\u001b[39m# instantiate a K-means\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/My_university/temp/CS405ML/lab/12GMM/Lab12.GMM%20Cluster.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     labels \u001b[39m=\u001b[39m kmeans\u001b[39m.\u001b[39;49mfit_predict(image_pixl)\u001b[39m# fit and get clustering result\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/My_university/temp/CS405ML/lab/12GMM/Lab12.GMM%20Cluster.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     initial_mus \u001b[39m=\u001b[39m kmeans\u001b[39m.\u001b[39mcluster_centers_\u001b[39m# get centroids\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/My_university/temp/CS405ML/lab/12GMM/Lab12.GMM%20Cluster.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     initial_priors, initial_covs \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\ml\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1069\u001b[0m, in \u001b[0;36m_BaseKMeans.fit_predict\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1046\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_predict\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1047\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute cluster centers and predict cluster index for each sample.\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \n\u001b[0;32m   1049\u001b[0m \u001b[39m    Convenience method; equivalent to calling fit(X) followed by\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1067\u001b[0m \u001b[39m        Index of the cluster each sample belongs to.\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1069\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, sample_weight\u001b[39m=\u001b[39;49msample_weight)\u001b[39m.\u001b[39mlabels_\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\ml\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\ml\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1475\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1447\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1448\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1449\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[0;32m   1450\u001b[0m \n\u001b[0;32m   1451\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1473\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1474\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1475\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m   1476\u001b[0m         X,\n\u001b[0;32m   1477\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1478\u001b[0m         dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32],\n\u001b[0;32m   1479\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1480\u001b[0m         copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy_x,\n\u001b[0;32m   1481\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1482\u001b[0m     )\n\u001b[0;32m   1484\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params_vs_input(X)\n\u001b[0;32m   1486\u001b[0m     random_state \u001b[39m=\u001b[39m check_random_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\ml\\Lib\\site-packages\\sklearn\\base.py:605\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    603\u001b[0m         out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    604\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 605\u001b[0m     out \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    606\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    607\u001b[0m     out \u001b[39m=\u001b[39m _check_y(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\ml\\Lib\\site-packages\\sklearn\\utils\\validation.py:930\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[39mif\u001b[39;00m ensure_2d:\n\u001b[0;32m    928\u001b[0m     \u001b[39m# If input is scalar raise error\u001b[39;00m\n\u001b[0;32m    929\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 930\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    931\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got scalar array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    932\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    933\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    934\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    935\u001b[0m         )\n\u001b[0;32m    936\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    937\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=nan.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "ih, iw, ic, image_norm = load(\"data/original/sample.png\")\n",
    "ncomp = 3\n",
    "iteration=500\n",
    "# init mu, prior and cov\n",
    "initial_mus, initial_priors, initial_covs = kmeans(ncomp, image_norm)\n",
    "\n",
    "# GMM\n",
    "print(\"GMM begins...\")\n",
    "gmm = GMM(ncomp, initial_mus, initial_covs, initial_priors)\n",
    "gmm.fit(image_norm, iteration)\n",
    "\n",
    "# visualize\n",
    "visualize(gmm, image_norm, ncomp, ih, iw)\n",
    "print(\"Finish!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample Result\n",
    "<img src=\"images/image-20220804223008133.png\" alt=\"image-20220804223008133\" style=\"zoom:67%;\" />\n",
    "<img src=\"images/image-20220804222915979.png\" alt=\"image-20220804222915979\" style=\"zoom: 67%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions(3 points)\n",
    "1. What are the strengths of GMM; when does it perform well?\n",
    "2. What are the weaknesses of GMM; when does it perform poorly?\n",
    "3. What makes GMM a good candidate for the clustering problem, if you have enough knowledge about the data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "773bfaa0e82744962f3138a2d7b2f007250d49f330da41a556809ccbcf17bfbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
